# âœ‹ Hand Gesture Recognition & Control

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)  
![OpenCV](https://img.shields.io/badge/OpenCV-Enabled-green)  
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange)  
![Mediapipe](https://img.shields.io/badge/Mediapipe-Hands-red)  
![License](https://img.shields.io/badge/License-MIT-lightgrey)

A **Hand Gesture Recognition System** built using **Mediapipe**, **OpenCV**, and **Deep Learning**.  
Recognizes static & dynamic gestures in realtime and demonstrates **Humanâ€“Machine Interaction** by controlling system **volume** with hand gestures.

---

## ğŸ“Œ Features
- ğŸ¥ **Realtime Gesture Recognition** (via webcam).  
- ğŸ–¼ï¸ **Static Image Prediction** (upload photo & detect gesture).  
- âœ‹ Multiple Gestures Supported:  
  - Palm (Open Hand)  
  - Thumbs Up  
  - Pointing (Index)  
  - Peace âœŒï¸  
  - Rock Sign ğŸ¤˜  
  - Call Me ğŸ¤™  
  - OK Sign ğŸ‘Œ  
  - Fist âœŠ  
- ğŸ”¢ **Finger Counting** (open vs. closed fingers).  
- ğŸ”Š **Volume Control** with thumbâ€“index distance (synced with system).  
- ğŸ“¡ Easily extendable to **media, IoT, or gaming control**.  

---

## ğŸ“‚ Dataset Collection
Custom dataset created using **webcam auto-capture**:
- Press **A** â†’ start capturing frames (500 samples per gesture).  
- Press **Q** â†’ quit.  
- Separate datasets for **Right Hand** and **Left Hand**.  
- Stored as **NumPy landmark arrays** from Mediapipeâ€™s 21 hand landmarks.  

---

## ğŸ§  Algorithms & Techniques
- **Hand Landmark Detection** â†’ Mediapipe Hands (21 3D landmarks).  
- **Feature Extraction** â†’ Normalized landmark positions, angles, distances.  
- **Finger Counting** â†’ Rule-based (open/closed detection).  
- **Classifier** â†’ Deep Neural Network (Keras Sequential model).  
- **Action Mapping** â†’ LabelEncoder maps gestures â†’ actions.  
- **System Volume Control** â†’ [pycaw](https://github.com/AndreMiras/pycaw) controls Windows audio.  

---

## ğŸ‹ï¸ Model Training
- Input: 63 features (x, y, z for 21 landmarks).  
- Model:  
  - Dense layers with ReLU  
  - Softmax output for gesture classification  
- Loss: Categorical Crossentropy  
- Optimizer: Adam  
- Output Files:  
  - `models/gesture_model.h5`  
  - `label_encoder.joblib`  

---

## âš™ï¸ Requirements
- Python **3.9+**  
- Libraries:
  ```bash
  pip install opencv-python mediapipe tensorflow scikit-learn joblib pycaw comtypes numpy

##ğŸš€ Usage
1ï¸âƒ£ Collect Dataset
