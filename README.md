# âœ‹ Hand Gesture Recognition & Control

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)  
![OpenCV](https://img.shields.io/badge/OpenCV-Enabled-green)  
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange)  
![Mediapipe](https://img.shields.io/badge/Mediapipe-Hands-red)  
![License](https://img.shields.io/badge/License-MIT-lightgrey)

A **Hand Gesture Recognition System** built using **Mediapipe**, **OpenCV**, and **Deep Learning**.  
Recognizes static & dynamic gestures in realtime and demonstrates **Humanâ€“Machine Interaction** by controlling system **volume** with hand gestures.

---

## ğŸ“Œ Features
- ğŸ¥ **Realtime Gesture Recognition** (via webcam).  
- ğŸ–¼ï¸ **Static Image Prediction** (upload photo & detect gesture).  
- âœ‹ Multiple Gestures Supported:  
  - Palm (Open Hand)  
  - Thumbs Up  
  - Pointing (Index)  
  - Peace âœŒï¸  
  - Rock Sign ğŸ¤˜  
  - Call Me ğŸ¤™  
  - OK Sign ğŸ‘Œ  
  - Fist âœŠ  
- ğŸ”¢ **Finger Counting** (open vs. closed fingers).  
- ğŸ”Š **Volume Control** with thumbâ€“index distance (synced with system).  
- ğŸ“¡ Easily extendable to **media, IoT, or gaming control**.  

---

## ğŸ“‚ Dataset Collection
Custom dataset created using **webcam auto-capture**:
- Press **A** â†’ start capturing frames (500 samples per gesture).  
- Press **Q** â†’ quit.  
- Separate datasets for **Right Hand** and **Left Hand**.  
- Stored as **NumPy landmark arrays** from Mediapipeâ€™s 21 hand landmarks.  

---

## ğŸ§  Algorithms & Techniques
- **Hand Landmark Detection** â†’ Mediapipe Hands (21 3D landmarks).  
- **Feature Extraction** â†’ Normalized landmark positions, angles, distances.  
- **Finger Counting** â†’ Rule-based (open/closed detection).  
- **Classifier** â†’ Deep Neural Network (Keras Sequential model).  
- **Action Mapping** â†’ LabelEncoder maps gestures â†’ actions.  
- **System Volume Control** â†’ [pycaw](https://github.com/AndreMiras/pycaw) controls Windows audio.  

---

## ğŸ‹ï¸ Model Training
- Input: 63 features (x, y, z for 21 landmarks).  
- Model:  
  - Dense layers with ReLU  
  - Softmax output for gesture classification  
- Loss: Categorical Crossentropy  
- Optimizer: Adam  
- Output Files:  
  - `models/gesture_model.h5`  
  - `label_encoder.joblib`  

---

## âš™ï¸ Requirements
- Python **3.9+**  
- Libraries:
  ```bash
  pip install opencv-python mediapipe tensorflow scikit-learn joblib pycaw comtypes numpy

---

## ğŸš€ Usage
**1ï¸âƒ£ Collect Dataset**
- python scripts/collect_data.py --gesture palm_right
- (repeat for all gestures & both hands)

**2ï¸âƒ£ Preprocess**
- python scripts/preprocess.py

**3ï¸âƒ£ Train**
- python scripts/train_model.py

**4ï¸âƒ£ Realtime Prediction (Webcam)**
- python scripts/realtime_predict.py

**5ï¸âƒ£ Static Image Prediction**
- python scripts/predict_image.py

**6ï¸âƒ£ Volume Control with Gestures**
- python scripts/volume_control.py

---

## ğŸ”® Future Enhancements

- **ğŸµ Media Controls â†’ Play, Pause, Next, Previous.**
- **ğŸ–±ï¸ Air Mouse â†’ Cursor & clicks using gestures.**
- **ğŸŒ IoT â†’ Smart home device control.**
- **ğŸ§ Sign Language â†’ Real-time translation into text/speech.**
- **ğŸ•¹ï¸ Gesture-Based Games.**

---

## ğŸ“œ Conclusion

- This project shows how **computer vision + deep learning** enable **real-time humanâ€“machine interaction.**
- It serves as a strong base for applications in **accessibility, IoT, robotics, AR/VR, and sign language recognition.**
